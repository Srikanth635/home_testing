# Programming Robotic Agents with Action Descriptions

Gayane Kazhoyan and Michael Beetz\* {kazhoyan, beetz}@cs.uni-bremen.de

*Abstract*â€” This paper tackles the problem of generalizing robot control programs over multiple objects, tasks and environments, based on the concept of action descriptions. These are abstract, general, semantic descriptions of an action that are augmented during execution with subsymbolic parameters specific to the context at hand. The parameters are inferred through reasoning rules, which extract the context from the action description and the belief state of the robot. The proposed system scales well with increasing number of reasoning rules required to support the knowledge-intensive manipulation tasks. The architecture combines the high-level robot control program with the reasoning engine in a modular way, thus improving the scalability of the system. The approach is validated in the context of setting a table with a PR2 robot.

#### I. INTRODUCTION

Today's leading-edge autonomous mobile manipulation robots have become capable of performing complex humanscale hand manipulation tasks such as folding clothes [1], making pancakes [2], preparing dishes [3] and conducting chemical experiments [4]. However, their control programs are tailored to the specific robots, tasks, objects and environment settings. Scaling the control programs towards more general settings, to automatically execute an action on different objects and in different scene and task contexts imposes huge challenges for their reasoning capabilities.

So far, only a small subset of reasoning capabilities needed by such robots is available. Specialized reasoning techniques such as motion planners can handle computation in geometric spaces well but do not take the semantic aspects of the task into account. On the other hand, the task planning techniques reason about fairly ambiguous models of actions and abstract away from *how* these models are realized. The implementation and analysis of control systems for robotic agents successfully performing realistic manipulation tasks makes apparent the need for the combination of the abovementioned approaches.

In realistic settings the manipulation instructions are typically stated by humans in a vague manner and need to be interpreted in order to perform them successfully. Consider, for example, a manipulation task such as "add milk to the dough". This task can be symbolically represented in the following way:

```
( p e r f o rm ( an a c t i o n
                     ( t y p e a d di n g )
                     ( theme ( a s u b s t a n c e ( t y p e mil k ) ) )
                     ( t a r g e t ( a s u b s t a n c e ( t y p e dough ) ) ) ) )
```
In order to interpret the task competently, a robotic agent has to infer that adding is to be accomplished through

\*The authors are with the Institute for Artificial Intelligence, University of Bremen, Germany.

*pouring*, that in order to pour the milk the robot has to grasp the *container* that contains the milk and pour it into the *container* that contains the dough. The robot also needs to infer *how to reach* for the container, *where to grasp*, how much *force* to apply in order to hold the container, and what the pouring *motion* should be. Thus, to produce a competent behavior the action representation above must be extended and modified to include the above-mentioned knowledge, for example, as following:

```
( p e r f o rm
   ( an a c t i o n
          ( t y p e p ourin g )
          ( s o u r c e ( an o b j e c t ( c o n t ai n s
                                                ( a s u b s t a n c e ( t y p e mil k ) ) ) ) )
          ( t a r g e t ( an o b j e c t ( c o n t ai n s
                                                ( a s u b s t a n c e ( t y p e dough ) ) ) ) )
          ( g r a s p - t y p e . . . )
          ( g r a s p - p o i n t s . . . )
          ( r e a c h i n g - t r a j e c t o r y . . . )
           . . . ) )
```
The inference tasks needed to fill in the knowledge gaps have to operate (1) at a symbolic level to apply semantic reasoning such as that milk on its own cannot be used as an object acted on in the context of a pouring action and a container should rather be used instead, together with (2) more fine-grained specialized reasoning techniques to infer the subsymbolic parameters of the action.

![](_page_0_Picture_14.jpeg)

Fig. 1. Different behaviors possibly generated by the same symbolic action representation.

A generic pouring action introduces even more ambiguities. Figure 1 visualizes different variations of behaviors and motions that accomplish a pouring action. The robotic agent needs to perform the action with one or two hands, using a tool, with additional constraints such as holding the lid while pouring, or with very specific motion skills as in the case of pouring beer into a glass. If we want to realize robotic agents mastering such general manipulation actions they have to know how pouring actions have to be performed to be successful, what their desired and undesired effects are, how to map desired effects into the action parameters that can be controlled, which tools to use and how, and so on.

The mastery of manipulation actions is extremely knowledge intensive and the research question of scalability towards such vast amounts of knowledge has received surprisingly little attention so far.

In this paper we introduce *action descriptions* as the key representational structures for reasoning about the successful execution of manipulation actions. Action descriptions are information resources that inform the selection of action parameterizations and that can be combined with the information about the task, object, situational context in order to choose the appropriate low-level parameters and constraints for grasps, motions, etc. We will argue and demonstrate that by making robot control programs capable of reasoning about and exploiting action descriptions, semantic reasoning methods can be applied in robot control that do not only generate abstract action plans but also parameterizations thereof necessary to execute the actions successfully.

The key contributions of this paper are the following:

- a novel way of parameterizing plans of autonomous mobile manipulation agents through *action descriptions*,
- a framework for combining knowledge from different sources in order to infer the missing knowledge preconditions of ambiguous action descriptions,
- a pipeline for transforming robot control programs parameterized by action descriptions into executable motions grounded in the robot's perception-action domain.

The proposed approach is validated in pouring and picking up tasks in the context of table setting with a PR2 robot.

#### II. CONCEPTS

#### *A. Action Descriptions*

Let us consider an example of an action of grasping an object with robot's left arm. On the subsymbolic level, this action represents a sequence of trajectories and gripper actions for robot's left arm that should successfully grasp a specific object in the environment. The action can be symbolically represented with the following action description: ( an a c t i o n ( t y p e g r a s p i n g )

```
( arm l e f t )
( o b j e c t - a c t e d - o n ( an o b j e c t ( t y p e cup ) ) )
( g o al ( o b j e c t - i n - h a n d o b j e c t - a c t e d - o n ) ) )
```
A general structure of an action description is outlined below.

- An is the *quantifier*, it specifies the quantity of description-satisfying solutions to consider. The quantifier can be a/an, the, all, etc.
- Action specifies the *type* of the entity described. In general, any entity in the robot control program can be described with the proposed representation. We have, however, identified the following entities for which it is most advantageous: objects, locations, motions and actions. For other entities, such as events, actors, etc., a traditional representation using Object-Oriented Programming paradigms proved itself more convenient.
- The key-value pairs are the *properties* of the description, which specify the constraints on the domain of possible groundings of the entity.

From the example action description it can be seen that the descriptions can be hierarchical, for example, actions can be described with objects, locations, motions or other actions with arbitrary nesting depth.

To represent complex actions that consist of multiple other actions or motions we consider the cognitive model of *forcedynamic events* [5]. According to that model, a manipulation action is comprised of different phases which are separated from each other through force-dynamic events. These events include making and breaking of contact between a human and an object or the object in hand and another object. For example, an action of picking up a cup involves phases such as grasping it and lifting it off a surface, and events such as making contact between the hand and the cup and breaking contact between the cup and the surface. Research shows that the force-dynamic events can be distinctly traced in a human brain and, thus, the fragmenting of an action into phases helps to understand observed actions of other humans.

In our system, the actions, their subactions, and complex activities, such as setting a table, are all represented through action descriptions. Motions are, however, a distinct entity, with the difference being that motion descriptions, when complete and unambiguous, should be physically executable on the given robot platform; actions, on the other hand, are goal-directed motions that are as independent of the specific platform as possible. An example of a high-level picking up action that is comprised of different action phases is given below.

|  |  |  | ( p e r f o rm |
|--|--|--|----------------|
|  |  |  |                |

| ( an a c t i o n | ( t y p e                      | p i c k i n g - u p ) |           |                          |
|------------------|--------------------------------|-----------------------|-----------|--------------------------|
|                  | ( o b j e c t                  | ( an o b j e c t      |           | ( t y p e cup ) ) )      |
|                  | ( arm l e f t )                |                       |           |                          |
|                  | ( p h a s e s ( an a c t i o n |                       | ( t y p e | r e a c h i n g )        |
|                  |                                |                       | ( l e f t | p o s e - 1 ) )          |
|                  |                                | ( an a c t i o n      |           | ( t y p e o p e ni n g ) |
|                  |                                |                       | ( l e f t | g r i p p e r ) )        |
|                  |                                | ( an a c t i o n      | ( t y p e | g r a s p i n g )        |
|                  |                                |                       | ( l e f t | p o s e - 2 ) )          |
|                  |                                | ( an a c t i o n      | ( t y p e | g r i p p i n g )        |
|                  |                                |                       | ( wit h   | l e f t )                |
|                  |                                |                       |           | ( f o r c e 50Nm) )      |
|                  |                                | ( an a c t i o n      | ( t y p e | l i f t i n g )          |
|                  |                                |                       | ( l e f t | p o s e - 3 ) ) ) ) )    |

#### *B. Reasoning Rules*

The step of going from an action description, as the one above, to a grounded motion, executable on a robot, is knowledge intensive. In Section I we listed a number of reasoning tasks necessary for successful execution of a picking up action. Computational routines, which can solve these tasks for a restricted set of input parameters, can be directly incorporated into the robot control program. This would, however, only enable the robot to perform the picking up action for a specific set of known objects, grasp types, the given robot platform or the specific environment. Often the result of the computational queries should also differ depending on the task: for example, in case of picking up an object the grasping configurations can differ if the object is for robot's personal use as opposed to for handing it over to the human. With the growth in the number of tasks and environments a particular robotic system can handle, the number of input parameters and computational routines grows exponentially. Thus, the straightforward approach of explicitly stating the different computational tasks and their parameterizations results in complex robot control programs.

We propose to provide the computational routines with symbolic context information to be incorporated during the subsymbolic data calculations, in the effort to make the routines more general. For example, the semantic constraint of holding a full container upright can be incorporated into the computation of the subsymbolic pose of the container in space. A useful additional advantage of such an approach is that it assists the robot in reasoning about its own code and possibly transforming it into a more efficient program, by semantically explaining why the particular object carrying pose is these specific subsymbolic values.

We call the reasoning tasks, which transform symbolic constraints into subsymbolic parameters of the controller while taking into account the context, *reasoning rules*.

For example, when the robot is faced with a task of picking up the cup in front of it, one of the questions it asks is: "What are the pregrasp and grasp configurations for this cup?" This can be represented using the following two queries:

```
a c t i o n g r a s p p o s e ( a c t i o n d e s c r i p t i o n , G pose ) ,
a c t i o n p r e g r a s p p o s e ( a c t i o n d e s c r i p t i o n , PG pose ) .
```
There will be multiple rules implementing the same query. One might call a computational routine such as a motion planner:

```
a c t i o n g r a s p p o s e ( a c t i o n d e s c r i p t i o n , G pose ) : -
   a c t i o n o b j e c t a c t e d o n ( a c t i o n d e s c r i p t i o n , Ob jec t ) ,
   o b j e c t g r a s p p o s e ( Ob jec t , G pose ) .
```
We use Prolog notation to represent reasoning rules: in the example, the first line is the *head* or the *declaration* of the rule, and the second and third lines are the *body*. Here action object acted on extracts the object acted on from the action description and binds it to the Object variable, and object grasp pose is a call to the motion planner.

An alternative implementation of action grasp pose would use experience data. Figure 2 visualizes this using recorded data of an already performed task within our episodic memory system [6].

![](_page_2_Picture_9.jpeg)

a c t i o n g r a s p p o s e ( *a c t i o n d e s c r i p t i o n* , G pose ) : s i m i l a r e x p e r i e n c e d a c t i o n ( *a c t i o n d e s c r i p t i o n* , A cti o n ) , a c t i o n e x p r e s s i o n ( Acti o n , [ an , a c t i o n , [ g r a s p p o s e , G pose ] ] ) .

? - a c t i o n g r a s p p o s e ( *a c t i o n d e s c r i p t i o n* , G pose ) , a c t i o n p r e g r a s p p o s e ( *a c t i o n d e s c r i p t i o n* , PG pose ) , show ( [ PG pose , r e d ] , [ G pose , bl u e ] ) .

Fig. 2. Querying action parameters based on action descriptions: the pregrasp pose is visualized in red, the grasp pose - in blue.

During execution, our system calls the grounding mechanisms to get one sample parameterization of the action to execute. If the inferred result does not lead to successful execution of the action, failure handling mechanisms call the next applicable grounding mechanism to generate a new parameterization sample. This will continue until the action is successfully executed or the number of retries is reached:

```
( l e t ( ( a c t i o n ( an a ct o n ( t y p e g r a s p i n g ) ( o b j e c t o b j e c t ) ) ) )
   ( w i t h - r e t r y - c o u n t e r s ( ( g r a s p - t r i e s 5 ) )
       ( w i t h - f a i l u r e - h a n d l i n g
               ( p e r f o rm a c t i o n )
           ( ( g r a s p i n g - f a i l e d ( )
               ( r e t r y g r a s p - t r i e s ( d i f f e r e n t - g r o u n d i n g a c t i o n ) ) ) ) ) ) )
```
We apply this trial and error approach heavily in our simulation and plan projection [7] environments and in contexts where a certain failure is not critical.

Ultimately, the specialized reasoning rules can be viewed as heuristics to speed up the sampling process and bootstrap the system. Once our episodic memory system contains enough samples to answer the knowledge queries, we will mostly rely on the rules learned from experience data.

More complex examples of reasoning rules are:

- if a container is filled and open then hold it upright,
- if an object can break then do not squeeze too hard,
- if the task context is pouring grasp at the center of mass,
- choose motion parameters that are predicted to succeed,
- grasp an object such that you have good visual feedback.

There is a number of important observations that can be made by examining these rules:

- many of the listed constraints are semantic,
- many apply in different situations and can be reused,
- the applicable rules might contradict each other.

Our system supports multiple implementations of reasoning rules with the same declaration, which enables combination of multiple relevant rules and dealing with inconsistencies.

The context of the reasoning rule is defined by the type of action at hand (e.g. picking up), the specific parameters of that action (e.g. pick up with the left or the right hand), the high-level action (e.g. picking up for my own use or to hand over to the human), and by the belief state of the robot. All of the above, except the belief state, will be explicitly specified in an action description by the end of grounding.

The context of the high-level action can be easily extracted due to the hierarchical nature of action descriptions: the high-level tasks of the robot are represented using action descriptions, which in their turn contain the symbolic descriptions of different phases of the action represented, which are themselves action descriptions.

#### III. GROUNDING SYMBOLIC DESCRIPTIONS

Because of the extremely large amount of reasoning rules that a robot control program could have, a search algorithm is necessary to find the ones applicable for grounding a particular action description in a particular context. We have chosen a light-weight Prolog engine to do that as the language already has a tree search algorithm and pattern matching to match the declarations of rules to the properties contained in the action descriptions. The executive in which the proposed concepts are implemented is the Cognitive Robot Abstract Machine (CRAM) [8].

We defined a grounded description rule, which has two arguments â€“ the symbolically described input entity and the result of grounding:

g r o u n d e d d e s c r i p t i o n ( D e s c r i p t i o n , G r o u n d e d D e s c ri pti o n ) .

There are multiple implementations of grounded description with different grounding mechanisms. The applicability check is performed automatically by the search algorithm by matching the expected and the given type of the action description and its other properties.

In our plans, the action tasks are triggered with the perform command, which grounds and executes action and motion descriptions. The latter are first grounded using the grounded description rule, then the plan associated with the particular action or motion is found by the search engine and called with the parameters of the grounded description. The plan associated with an action contains other perform calls, thus implementing a task hierarchy. The plans associated with motions are on the lowest level of the hierarchy and implement functionality specific for the particular robot platform.

Object descriptions are grounded with the detect command using our perception system [9]. The input abstract symbolic description can look like the following:

( d e t e c t ( an o b j e c t ( t y p e cup ) ) )

and the result would be the same object description, augmented with mixed symbolic and subsymbolic parameters of the object in robot's current environment.

Locations are grounded with the reference command using geometric reasoning mechanisms [10]. They can resolve into a pose, a region of poses or a tuple of pose and standard deviation to represent measurement uncertainty:

```
( r e f e r e n c e ( a l o c a t i o n ( r e a c h a b l e - f o r p r 2 )
                                           ( o b j e c t p e r c e i v e d - o b j e c t ) ) )
```
Motions are augmented with subsymbolic data and executed by low-level functions that are specific for the particular robot platform, e.g., a call to a certain controller [11]: ( p e r f o rm ( a m oti o n ( t y p e c a r t e s i a n - c o n s t r a i n t s ) ( l e f t p o s e - 1 ) ) )

Atomic actions resolve into plans that include calls to perform motion descriptions.

High-level actions are augmented with the description of their phases. The parameterization of the individual phases can be symbolic or it can already be grounded if the values are dependent on the high-level action context, e.g., the grasping pose for a tool in the using-a-tool vs. hand-over actions.

Using our action representation model based on the forcedynamic events, the different phases of high-level actions and their parameters can be learned, e.g., from human demonstrations in simulated environments [11].

The application of reasoning rules is implicit in the robot control program, which improves the readability and maintainability of the code. This separates the reasoning for calculating specific motion parameters from the high-level plan such that the plan would stay general and scalable.

#### IV. EXAMPLES OF REASONING RULES

This section demonstrates some of the reasoning rules, using the picking up and pouring actions as examples. The knowledge to answer the queries can come from multiple sources: specialized computing routines that perform, e.g., grasp configuration calculations, reasoning over the belief state, using the past experiences of the robot, etc.

Here is an example of a simple rule to infer the free hands of the robot based on its belief state:

```
f r e e a r m s ( Robot , Arm ) : -
```

```
f i n d a l l (A, n ot ( o b j e c t i n h a n d ( Robot , Obj , A ) ) , Arms ) ,
member ( Arm , Arms ) .
```
The ambiguous picking up action description is then grounded using a rule similar to the following, where we skipped details for the sake of clarity:

```
g r o u n d e d d e s c r i p t i o n ( a c t i o n d e s c r i p t i o n , Gr ounded ac ti on ) : -
   d e s c r i p t i o n t y p e ( a c t i o n d e s c r i p t i o n , ' a c t i o n ' ) ,
   r o b o t ( Robot ) ,
   f r e e a r m s ( Robot , Arm) ,
   o b j e c t a c t e d o n ( a c t i o n d e s c r i p t i o n , Ob jec t ) ,
   o b j e c t t y p e ( Ob jec t , O b j e c t t y p e ) ,
   o b j e c t p o s e ( Ob jec t , O b j e c t p o s e ) ,
   g r a s p t y p e ( O b j e c t t y p e , Gr a sp t ype ) ,
   a c t i o n t y p e ( a c t i o n d e s c r i p t i o n , A c ti o n t y p e ) ,
   g r a s p p o s e ( A c ti o n t y p e , O b j e c t t y p e , Gr a sp t ype , Arm ,
                      G pose ) ,
   g r i p f o r c e ( O b j e c t t y p e , Gr a sp t ype , G ri p f o r c e ) ,
   . . .
   p h a s e s ( A c ti o n t y p e , G pose , . . . , Ph a se s ) ,
   a u g m e n t w i t h p h a s e s ( a c t i o n d e s c r i p t i o n , Ph a se s ,
                                     Gr ounded ac ti on ) .
```
First, the system infers which arm is free such that it could be used to pick up the object. Then it extracts the context information from the action description, in our case that is the type of the action and that of the object. Finally, it infers the missing low-level parameters of the action and augments the action description with its corresponding phases.

For the pouring action a robot with an arbitrary number of arms could hold either both the source and target object in its hands, or only the source object (see Figure 4), which will result in different controller behaviors. To infer which robot arms are involved in the action the following reasoning rules can be applied:

```
a c t i o n a r m s ( a c t i o n d e s c r i p t i o n , Arms) : -
   r o b o t ( Robot ) ,
   o b j e c t i n h a n d ( Robot , p e r c ei v e d - t a r g e t - o bj , ArmTarget ) ,
   o b j e c t i n h a n d ( Robot , p e r c ei v e d - s o u r c e - o bj , ArmSource ) ,
  Arms i s [ ArmTarget , ArmSource ] .
a c t i o n a r m s ( a c t i o n d e s c r i p t i o n , Arm) : -
   r o b o t ( Robot ) ,
```
n ot ( o b j e c t i n h a n d ( Robot , *p e r c ei v e d - t a r g e t - o bj* , A ) ) , o b j e c t i n h a n d ( Robot , *p e r c ei v e d - s o u r c e - o bj* , Arm ) .

Notice that there are two rules implementing the same query. If the target object is not in the hand, the first rule will fail, and the second rule will succeed. As with the picking up action, here as well the context is derived from the action description itself and from the belief state of the robot.

#### V. EXPERIMENTAL ANALYSIS

Our approach was evaluated in a table setting plan, which involved going, perceiving, picking up, placing and pouring.

#### *A. Picking up Example*

The objects supported in the picking up experiment were cup, plate, fork, knife and bottle. The plan of picking up an object started with a perception task of detecting an object of a certain type:

(detect (an object (type *object-type)))*

The result of this command was a grounded into the environment object description, e.g.:

```
(an object (type cup)
             (pose pose-J) (cad-model "cup_eco_orange")
             (color yellow red black)
             (cl uster-id 2.0) ... )
```
Having found the object, the plan proceeded to reposition the robot to a reachable location:

```
(perform (an action (type going) (to (a location
                                 (reachable-for pr2)
                                 (object perceived-object )))))
```
where perceived-object is the result from the previous command.

Finally, the plan commanded the robot to perform the picking up action. The initial action description was generic:

```
(perform (an action
                       (type picking-up) (object perceived-object) (goal (object-in-hand perceived-object)))
```
The reasoning rules grounded the action description into robot's environment and got the following result:

```
(an action
                              (an act i on
                              ( an act j on
                              (an act i on
(perform (an action
                       (type picki ng-up) (object perceived-object) (goal (object-in-hand perceived-object)) (arm left)
                (phases (an actioo (type reaching) (left pose-I))
                                                  (type opening) (left gripper)) (type grasping) (left pose-2)) (type gripping)
                                                  (with left)
                                                  (force 50Nm))
                                                  (type lifting)
                                                  (left pose-3 )))))
```
The plan that performed the resulting action description executed the phases sequentially, in the order that they had in the action description. This is the default behavior, whereas our system additionally supports any plan written in the concurrent-reactive CRAM Plan Language [8].

![](_page_4_Picture_12.jpeg)

Fig. 3. PR2 robot picking up different objects.

The high-level plan looked exactly the same for all the objects involved, which resulted in a high-level plan that is, despite its generality, successfully executable on a real robot owing to the implicit application of reasoning rules.

## *B. Pouring Example*

The action description of the pouring action looks like the following:

(perform (an action (type pouring) (target *perceived-large/-object)* (pour-volume 100ml))

The variety in the pouring action was within the target object location: it could either be (1) standing on a surface, (2) be in a human's hand, or (3) in the robot's hand.

![](_page_4_Picture_19.jpeg)

Fig. 4. PR2 robot pouring with different target locations.

From the point of view of the plan, pouring into an object on a surface and in the hand of the human is equivalent, as the only parameter that changed was the pose of the object. The interesting to examine difference is in the arms involved in the action: if the robot had the target object also in the hand it could choose a configuration that would be most convenient for its arms. For executing the pouring motions we use a constraint-based controller, so one of the reasoning rules queries a model that calculates the corresponding constraints based on the relative positioning of the objects involved and other parameters, such as the pouring volume [11).

The resulting action description for a two-handed pouring action looked like the following:

```
( perform
  (an action
       (type pouring)
       (source (an object (type bottle) â€¢â€¢. ))
       (target perceived-target-object) (pour-volume 100ml)
       (arm (left rigbt)) (phases (an action
                     (type approaching) (constraints .))
                (an action
                     (type ti I ti ng-down)
                     ( constraints
                        (( s-top_lefLs-bottom 0.160 0.164)
                         (s-top_behind_Ltop -0.002 0.005))))
                (an action
                     (type tilting-back) (constraints ... )))))
```
### VI. RELATED WORK

One of the main efforts of this paper is to ease the mapping from the symbolic domain of abstract representations to the subsymbolic concepts from robot's perception-action loop. A system that addresses a similar problem is the Hierarchical Planning in the Now (HPN) architecture [12] which is a system that combines a task planner with a motion planner. That is implemented by using *geometric suggesters,* which give fast and approximate solutions to computational problems. The system constructs the plans at an abstract level and recursively plans and executes the actions while calculating their parameters on the fly. In that regard, it is similar to our system, where plans are stated in a general way and the parameters are calculated at run-time through implicit application of reasoning rules, the role of which in our system is similar to the geometric suggesters in the HPN architecture. The difference is that the amount of context information that the HPN system can handle is limited: the approach relies on a small set of plausible input parameters for the operators used to construct the plan, whereas our system is explicitly targeting at a large amount of contextdependent parameters due to the knowledge-intensive nature of complex human-scale manipulation tasks.

Another system, similar to ours, is Tell me Dave [13], which grounds vague natural language instructions to executable manipulation actions while taking context into account. Here, again, the particular atomic actions have only a very limited amount of parameters, so the system is argued to be able to scale to new objects or similar actions but it does not scale with new types of knowledge and semantic constraints over the manipulation actions.

A number of state-machine-based approaches has also been developed recently. One of them is the ROS Commander (ROSCo) [14], which is a hierarchical finite state machine that provides generic parameterized building blocks for the users to build their own robot behaviors. The system demonstrates execution of variety of tasks in realistic home environments but it suffers from the general limitation that the state-machine-based approaches have, namely, that they do not generalize well to new environment contexts.

Classical task planning approaches based on situation calculus and PDDL-like representations [15] and generalized planning methods allow for stating plans in a general manner, however, abstracting away from how the actions are realized. We argue that choosing the correct parameterization of actions is extremely important for the success of action execution. Our high-level representation of plans extends the classical symbolic models of actions with the notion of motion parameterizations necessary for achieving the desired effects of the actions.

#### VII. CONCLUSION AND FUTURE WORK

We introduced a concept of action descriptions and demonstrated how they can be applied when writing robot control programs. We showed how our approach combines symbolic and subsymbolic reasoning tasks with the robot plans without the loss of generality of the plan. We argued that our approach improves the scalability of robot control programs over the large domain of autonomous manipulation tasks by incorporating multiple reasoning rules for inferring the parameters of actions at hand. We evaluated our approach on the example of pouring and picking up plans with different objects in the context of table setting.

Our system enables a combination of multiple rules for inferring the missing knowledge preconditions of the same action. The results of these rules could potentially be inconsistent. In future, we plan to develop on this idea and tackle this problem by transforming our reasoning engine into an expert system, i.e. a system where different experts generate hypotheses as an answer to the same query, which are then combined together [16]. The basis will be our previous work where we successfully applied the expert system approach in the area of computer vision [9]. One way to evaluate the answers from different experts would be to test the plan by simulating it with the given parameters.

#### ACKNOWLEDGMENTS

This work was supported in part by the EU FP7 Project *RoboHow* (grant number 288533).

#### REFERENCES

- [1] S. Miller, J. Van Den Berg, M. Fritz, T. Darrell, K. Goldberg, and P. Abbeel, "A geometric approach to robotic laundry folding," *The International Journal of Robotics Research*, vol. 31, no. 2, 2012.
- [2] M. Beetz, U. Klank, I. Kresse, A. Maldonado, L. Mosenlechner, Â¨ D. Pangercic, T. Ruhr, and M. Tenorth, "Robotic Roommates Making Â¨ Pancakes," in *11th IEEE-RAS International Conference on Humanoid Robots*, 2011.
- [3] K. Okada, T. Ogura, A. Haneda, J. Fujimoto, F. Gravot, and M. Inaba, "Humanoid motion generation system on hrp2-jsk for daily life environment," in *IEEE International Conference Mechatronics and Automation*, 2005.
- [4] G. Lisca, D. Nyga, F. Balint-Bencz Â´ edi, H. Langer, and M. Beetz, Â´ "Towards Robots Conducting Chemical Experiments," in *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2015.
- [5] J. R. Flanagan, M. C. Bowman, and R. S. Johansson, "Control strategies in object manipulation tasks," *Current Opinion in Neurobiology*, 2006.
- [6] M. Beetz, M. Tenorth, and J. Winkler, "Open-EASE a knowledge processing service for robots and robotics/ai researchers," in *IEEE International Conference on Robotics and Automation (ICRA)*, 2015, Finalist for the Best Cognitive Robotics Paper Award.
- [7] L. Mosenlechner and M. Beetz, "Fast temporal projection using Â¨ accurate physics-based geometric reasoning," in *IEEE International Conference on Robotics and Automation (ICRA)*, 2013.
- [8] M. Beetz, L. Mosenlechner, and M. Tenorth, "CRAM A Cognitive Â¨ Robot Abstract Machine for Everyday Manipulation in Human Environments," in *Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems*, 2010.
- [9] M. Beetz, F. Balint-Benczedi, N. Blodow, D. Nyga, T. Wiedemeyer, and Z.-C. Marton, "RoboSherlock: Unstructured Information Processing for Robot Perception," in *IEEE International Conference on Robotics and Automation (ICRA)*, 2015, Best Service Robotics Paper Award.
- [10] L. Mosenlechner and M. Beetz, "Parameterizing Actions to have Â¨ the Appropriate Effects," in *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2011.
- [11] Z. Fang, G. Bartels, and M. Beetz, "Learning models for constraintbased motion parameterization from interactive physics-based simulation," in *International Conference on Intelligent Robots and Systems (IROS)*, 2016.
- [12] L. P. Kaelbling and T. Lozano-Perez, "Hierarchical task and motion Â´ planning in the now," in *Robotics and Automation (ICRA), 2011 IEEE International Conference on*, 2011.
- [13] D. K. Misra, J. Sung, K. Lee, and A. Saxena, "Tell me dave: Contextsensitive grounding of natural language to manipulation instructions," *The International Journal of Robotics Research*, 2015.
- [14] H. Nguyen, M. Ciocarlie, K. Hsiao, and C. C. Kemp, "Ros commander (rosco): Behavior creation for home robots," in *Robotics and Automation (ICRA), 2013 IEEE International Conference on*, 2013.
- [15] H. J. Levesque, R. Reiter, Y. Lesperance, F. Lin, and R. B. Scherl, Â´ "Golog: A logic programming language for dynamic domains," *The Journal of Logic Programming*, 1997.
- [16] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. Prager, *et al.*, "Building watson: An overview of the deepqa project," *AI magazine*, vol. 31, no. 3, 2010.